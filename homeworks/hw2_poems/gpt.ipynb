{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMukUUTZgd9fS70Vkp7nq+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vejber/nn-zero-to-hero/blob/master/homeworks/hw2_poems/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск без изменений, с датасетом стихов на русском:"
      ],
      "metadata": {
        "id": "N7H2iLmyyEPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmZR0B1VP92K",
        "outputId": "eb47addc-a8d2-45d5-e5db-1b846560a7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.854294 M parameters\n",
            "step 0: train loss 5.2275, val loss 5.2390\n",
            "step 500: train loss 2.0231, val loss 2.0690\n",
            "step 1000: train loss 1.6376, val loss 1.8169\n",
            "step 1500: train loss 1.4485, val loss 1.7095\n",
            "step 2000: train loss 1.3059, val loss 1.6349\n",
            "step 2500: train loss 1.2061, val loss 1.6360\n",
            "step 3000: train loss 1.1147, val loss 1.6518\n",
            "step 3500: train loss 1.0338, val loss 1.6756\n",
            "step 4000: train loss 0.9447, val loss 1.7275\n",
            "step 4500: train loss 0.8607, val loss 1.7809\n",
            "step 4999: train loss 0.7832, val loss 1.8389\n",
            "  perplexity_train  perplexity_val\n",
            "0   tensor(2.1885)  tensor(6.2898)\n",
            "\n",
            "не с клоней волной —\n",
            "   На воротах.\n",
            " \n",
            "   Я здесь молчала и течет… волей,\n",
            "   Ты тень услыхаешь моем.\n",
            " \n",
            "   – Вы ли тебя улыбку первый луж,\n",
            "   Во рта ветворенском, —\n",
            "   Сердце Ты темна. Споряют, сердце мое, подружка!\n",
            " \n",
            " \n",
            "   Царь и напоризна корет,\n",
            "   Сказал бы мого и грусть, – оторвая твоему.\n",
            "   Ты знала, последий мой, один ружь!\n",
            "   Ты хороша, Ревностью не выйду!\n",
            " \n",
            "   Ты смелаешь родиной чьей ногою\n",
            "   – Москва, Вы все прислил чружился бы турзаной!\n",
            "   Как московский месяц Аарелопстей,\n",
            "   В круже рос\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from urllib import request\n",
        "import pandas as pd\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 256  # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with request.urlopen('https://raw.githubusercontent.com/Vejber/nn-zero-to-hero/master/homeworks/hw2_poems/poems.txt') as f:\n",
        "    text = f.read().decode()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "# encoder: take a string, output a list of integers\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "# decoder: take a list of integers, output a string\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if iter == max_iters - 1:\n",
        "            # Создаем DataFrame с первой строкой данных\n",
        "            df = pd.DataFrame({'perplexity_train': [torch.exp(losses['train'])], 'perplexity_val': [torch.exp(losses['val'])]})\n",
        "            print(df)\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print perplexity\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP9ZAUM35w3u",
        "outputId": "8424be69-451e-49f0-9d7b-796cd04140ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   perplexity_train  perplexity_val\n",
            "0            2.1885          6.2898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже изменены параметры: функция активации (ELU), 8 layers, dff = 4608, H = 18."
      ],
      "metadata": {
        "id": "rzPA1RCYP6an"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b3544e-2872-4d03-d8c0-52faf5dd712f",
        "id": "KRQeJWZ2PyuS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33.226134 M parameters\n",
            "step 0: train loss 5.0906, val loss 5.0890\n",
            "step 500: train loss 2.2227, val loss 2.2511\n",
            "step 1000: train loss 1.9366, val loss 2.0095\n",
            "step 1500: train loss 1.8086, val loss 1.9247\n",
            "step 2000: train loss 1.7191, val loss 1.8630\n",
            "step 2500: train loss 1.6441, val loss 1.8239\n",
            "step 3000: train loss 1.5691, val loss 1.7781\n",
            "step 3500: train loss 1.5037, val loss 1.7312\n",
            "step 4000: train loss 1.4377, val loss 1.7049\n",
            "step 4500: train loss 1.3806, val loss 1.6754\n",
            "step 4999: train loss 1.3291, val loss 1.6602\n",
            "  perplexity_train  perplexity_val\n",
            "0   tensor(3.7776)  tensor(5.2602)\n",
            "\n",
            "   Кивает не завеслый дножа\n",
            "   Одинскими, вселенными далёкся,\n",
            "   Стрекая говор ега!\n",
            " \n",
            "   10 октября 1919\n",
            "“Даль И Когда Страши…”\n",
            "   Да, когда страшива, к земную —\n",
            "   Воркому пролять еще.\n",
            "   Под рекою больботы.\n",
            "   Мнё – колений!\n",
            " \n",
            "   Улыбельно одыши его\n",
            "   Бурь нам нежена,\n",
            "   Не видем, а нежной один —\n",
            "   От нежно я,\n",
            "   В верике пир и от ведь!\n",
            " \n",
            "   Уж и на гробу ваших не отвечных,\n",
            "   В паритете?.. Жених люблю\n",
            "   Конечный случайный зармы,\n",
            "   Горельностью одной барманы,\n",
            "   С шермыной долгое серые Дну\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from urllib import request\n",
        "import pandas as pd\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 256  # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "\n",
        "# changed block\n",
        "n_head = 18\n",
        "n_layer = 8\n",
        "n_dff = 4608\n",
        "# changed block end\n",
        "\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with request.urlopen('https://raw.githubusercontent.com/Vejber/nn-zero-to-hero/master/homeworks/hw2_poems/poems.txt') as f:\n",
        "    text = f.read().decode()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "# encoder: take a string, output a list of integers\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "# decoder: take a list of integers, output a string\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_dff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_dff),\n",
        "            # new activation function:\n",
        "            nn.ELU(),\n",
        "            nn.Linear(n_dff, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd, n_dff)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "perplexity_log = f'perplexity_train,perplexity_val\\n'\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if iter == max_iters - 1:\n",
        "          # Создаем DataFrame с первой строкой данных\n",
        "            data = pd.DataFrame({'perplexity_train': [torch.exp(losses['train'])], 'perplexity_val': [torch.exp(losses['val'])]})\n",
        "            print(data)\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сравнение значений perplexity до и после изменений параметров и функции активации"
      ],
      "metadata": {
        "id": "jQpnyZUkx0cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Чтение данных из CSV файла\n",
        "file_path = '/content/perplexity.csv'\n",
        "data = pd.read_csv(file_path, delimiter=';', decimal=',')\n",
        "\n",
        "# Создание столбчатой диаграммы\n",
        "fig, ax = plt.subplots()\n",
        "index = [0, 1]\n",
        "bar_width = 0.35\n",
        "opacity = 0.8\n",
        "\n",
        "# Получение значений\n",
        "perplexity_train_values = data['perplexity_train']\n",
        "perplexity_val_values = data['perplexity_val']\n",
        "\n",
        "# Расчет позиций для столбцов\n",
        "train_positions = [x - bar_width/2 for x in index]\n",
        "val_positions = [x + bar_width/2 for x in index]\n",
        "\n",
        "# Рисование столбцов\n",
        "rects1 = ax.bar(train_positions, perplexity_train_values, bar_width, alpha=opacity, color='b', label='Perplexity Train')\n",
        "rects2 = ax.bar(val_positions, perplexity_val_values, bar_width, alpha=opacity, color='r', label='Perplexity Val')\n",
        "\n",
        "# Настройка осей\n",
        "ax.set_xlabel('Measurements')\n",
        "ax.set_ylabel('Values')\n",
        "ax.set_title('Perplexity Train and Val Comparison')\n",
        "ax.set_xticks(index)\n",
        "ax.set_xticklabels(['before', 'after'])\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9F7xI_quxFWp",
        "outputId": "d9e25a7c-0775-420b-85b5-35ded7ea7594"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}